{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snZVFgU91tAW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umqeq6w632Ng"
   },
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 200\n",
    "\n",
    "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
    "\n",
    "\n",
    "def preprocess(sequences, labels):\n",
    "    return sequences, labels.astype(np.int32)\n",
    "\n",
    "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
    "test_sequences, test_labels = preprocess(test_sequences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHJA6HJm33km"
   },
   "outputs": [],
   "source": [
    "vocabulary = tf.keras.datasets.imdb.get_word_index()\n",
    "char_to_ind = vocabulary\n",
    "ind_to_char = {ind: char for (char, ind) in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5fercgq35v5"
   },
   "outputs": [],
   "source": [
    "# remember this? doesn't work...\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxmJFX1B4MZ4"
   },
   "outputs": [],
   "source": [
    "# we can create a dataset from a python generator. first, we have to write the generator\n",
    "# this is a very simple one, but we could execute arbitrary python code in here\n",
    "# (say, loading files from disk and preparing the loaded inputs somehow)\n",
    "def gen():\n",
    "    for sequence, label in zip(train_sequences, train_labels):\n",
    "        yield sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Lq6XE4j4gay"
   },
   "outputs": [],
   "source": [
    "# we have to tell TF what to expect from the generator (\"Tensor Specification\")\n",
    "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
    "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
    "\n",
    "# regular .batch wouldn't work because the inputs are different length.\n",
    "# padded batch automatically pads all elements in the batch to the longest length\n",
    "# per dimension.\n",
    "# you can also specify different shapes and padding values other than 0.\n",
    "# padding is always \"post\"\n",
    "train_data = train_data.padded_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUx7O2Ha4ifV"
   },
   "outputs": [],
   "source": [
    "for sequence, label in train_data:\n",
    "    print(sequence.shape, label.shape)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVgxkqBIoXWe"
   },
   "outputs": [],
   "source": [
    "# we have to tell TF what to expect from the generator (\"Tensor Specification\")\n",
    "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
    "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
    "\n",
    "# alternatively, we can use bucketing. the idea is to define buckets for specific\n",
    "# sequence lengths, and put all sequences in their corresponding bucket.\n",
    "# when a batch is requested, first a bucket is selected and then all elements of\n",
    "# the batch are taken from this bucket.\n",
    "# this guarantees that all elements in a batch are roughly the same length,\n",
    "# minimizing the amount of padding.\n",
    "\n",
    "# here is an example with buckets in steps of 50. all sequences above length 500\n",
    "# end up in the same bucket. same for sequences below length 50.\n",
    "# do note that I by no means claim that this is a \"good\" bucketing. play around with it!\n",
    "buckets = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "bucket_batch_size = [32] * (len(buckets) + 1)\n",
    "train_data = train_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0],\n",
    "                                                  bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE!!\n",
    "# you should probably still remove very long sequences (longer than some cutoff)\n",
    "# before converting to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Bj3tdaWpCZz"
   },
   "outputs": [],
   "source": [
    "# compare the average batch shapes with the padded_batch example. there, batches are\n",
    "# often length 800 or so because the longest sequence in the batch happened to\n",
    "# have that length.\n",
    "# with bucketing, we get many much smaller batches, meaning more efficient training.\n",
    "for sequence, label in train_data:\n",
    "    print(sequence.shape, label.shape)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R5z3NdMAwD9"
   },
   "outputs": [],
   "source": [
    "# here's a very simple toy example for a keras lstm\n",
    "# the \"hidden dimensions\" are just randomly chosen. \n",
    "# you probably don't want to use a hidden size of 12 =) (but maybe it's actually really good?)\n",
    "\n",
    "\n",
    "# embedding comes first to replace one-hot vectors. \n",
    "#    mask_zero=True to prevent computations on padded time steps.\n",
    "# then an arbitrary number of RNN layers.\n",
    "# deeper RNN layers take as input sequence the state sequence of the layer before,\n",
    "# so all layers except the last one should return_sequences=True\n",
    "# finally, a Dense layer for the output, since the output computation is *not*\n",
    "# included in the RNN cells; all cells provided by Keras only compute the states\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 20, mask_zero=True), \n",
    "                             tf.keras.layers.LSTM(12, return_sequences=True),\n",
    "                             tf.keras.layers.LSTM(15),\n",
    "                             tf.keras.layers.Dense(1)])\n",
    "\n",
    "\n",
    "# FYI, the third line is the same as the first two lines together.\n",
    "# the second option can use a much more efficient implementation, it will be SOOO much faster.\n",
    "# try it yourself!\n",
    "#rnn_cell = tf.keras.layers.LSTMCell(12)\n",
    "#rnn = tf.keras.layers.RNN(rnn_cell, return_sequences=False)\n",
    "rnn = tf.keras.layers.LSTM(12, return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQvxblepEoDt"
   },
   "outputs": [],
   "source": [
    "# calling RNN layers is easy!\n",
    "one_hot_batch = tf.one_hot(sequence, depth=max_words)\n",
    "rnn(one_hot_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piSLVKmSEyP2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnns_part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
